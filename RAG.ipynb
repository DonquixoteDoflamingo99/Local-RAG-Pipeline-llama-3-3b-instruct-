{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc65087-b20e-40c5-bd36-504301f25163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84912ea8-39f4-4049-adf5-d6ca8d439f4a",
   "metadata": {},
   "source": [
    "## Document Text processing and embedding creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264c6b6-597a-4e6a-8564-328aa967726c",
   "metadata": {},
   "source": [
    "Steps : \n",
    "1. Open a pdf\n",
    "2. Format the text for the embedding model (split into chunk of text)\n",
    "3. Embed text chunks with embedding model\n",
    "4. Save embeddings to a file for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12951a2b-c568-443d-bbd1-b357075b08eb",
   "metadata": {},
   "source": [
    "Requirements : \n",
    "1. pdf \n",
    "2. embedding model of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b58c8f-c95a-4107-9ba9-6f6e432a419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/yashb/Downloads/RAG testing/pdfs/Book-2Designing-data-intensive-applications.pdf exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "#the pdf document path\n",
    "pdf_doc = 'C:/Users/yashb/Downloads/RAG testing/pdfs/Book-2Designing-data-intensive-applications.pdf'\n",
    "\n",
    "if not os.path.exists(pdf_doc):\n",
    "    print(\"[INFO] file does not exist, Downloading using the url....\")\n",
    "    url = \"Put the download url of the pdf\"\n",
    "    filename = pdf_doc\n",
    "\n",
    "    #get request for pdf download\n",
    "    response = requests.get(url)\n",
    "    #check if the get request was successful\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\" [INFO] The file have been downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Request ended with status code {respose.status_code}\")\n",
    "else:\n",
    "    print(f\"File {pdf_doc} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad17513e-5045-4b4e-b84c-ff88e39eda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashb\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "613it [00:01, 452.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "   \n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove hyphenation at line breaks\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    \n",
    "    # Merge lines (replace single newlines with space)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove footers like \"216\\n|\\nChapter 6: Partitioning or opposite\"\n",
    "    text = re.sub(r'\\d+\\s*\\n\\|\\n.*?$', '', text.strip(), flags=re.MULTILINE)\n",
    "    #text = re.sub(r\"\\s*\\|\\s*\\d+\\s*$\", \"\", text)\n",
    "    text = re.sub(r\"^\\s*\\d+\\s*\\n\\s*\\|\\s*\\n\\s*Chapter\\s+\\d+:.*(?:\\n)?\", '', text.strip(), flags=re.MULTILINE)\n",
    "\n",
    "    # Final strip\n",
    "    return text.strip()\n",
    "\n",
    "#or till doc.page_count currently removing the unimportant pages from this book\n",
    "def open_read_clean_pdf(pdf_doc: str) -> list[dict]:\n",
    "    resource = pymupdf.open(pdf_doc)\n",
    "    all_text = []\n",
    "    for page_num, page in tqdm(enumerate(resource)):\n",
    "        text = page.get_text()\n",
    "        text = clean_text(text=text)\n",
    "        all_text.append(\n",
    "            {\n",
    "                'page_no': page_num - 21, #There is unuseful stuff till page 23 so you can do whatever with this, here the page number starts at page 22 or 23\n",
    "                'page_char_cnt': len(text),\n",
    "                'page_word_cnt': len(text.split(\" \")),\n",
    "                'page_sentence_cnt': len(text.split(\". \")),\n",
    "                'page_token_cnt': len(text)/4,   #English - token is approx 4 characters\n",
    "                'text': text                \n",
    "            }\n",
    "        )\n",
    "    return all_text\n",
    "\n",
    "all_text = open_read_clean_pdf(pdf_doc = pdf_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d6656b-75b5-4265-a7f1-037b520648eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': 4,\n",
       " 'page_char_cnt': 2919,\n",
       " 'page_word_cnt': 464,\n",
       " 'page_sentence_cnt': 22,\n",
       " 'page_token_cnt': 729.75,\n",
       " 'text': 'But reality is not that simple. There are many database systems with different charac‐ teristics, because different applications have different requirements. There are vari‐ ous approaches to caching, several ways of building search indexes, and so on. When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone. This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications. We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics. In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems. We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters. In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application. Thinking About Data Systems We typically think of databases, queues, caches, etc. as being very different categories of tools. Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations. So why should we lump them all together under an umbrella term like data systems? Many new tools for data storage and processing have emerged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1]. For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka). The boundaries between the categories are becoming blurred. Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs. Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code. For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database. Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters). 4 | Chapter 1: Reliable, Scalable, and Maintainable Applications'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[25]\n",
    "#len(all_text)\n",
    "\n",
    "#resource = pymupdf.open(pdf_doc)\n",
    "#for page in range(27,28):\n",
    "#    page = doc.load_page(page)\n",
    "#    text = page.get_text()\n",
    "#    print(text)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03689e3c-b78e-40ca-a744-b0fe82886355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt</th>\n",
       "      <th>page_token_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>2303.140294</td>\n",
       "      <td>371.422512</td>\n",
       "      <td>17.084829</td>\n",
       "      <td>575.785073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>177.102136</td>\n",
       "      <td>703.855066</td>\n",
       "      <td>118.008208</td>\n",
       "      <td>14.287294</td>\n",
       "      <td>175.963767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>506.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>2560.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>2798.000000</td>\n",
       "      <td>457.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>699.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>591.000000</td>\n",
       "      <td>3245.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>811.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          page_no  page_char_cnt  page_word_cnt  page_sentence_cnt  \\\n",
       "count  613.000000     613.000000     613.000000         613.000000   \n",
       "mean   285.000000    2303.140294     371.422512          17.084829   \n",
       "std    177.102136     703.855066     118.008208          14.287294   \n",
       "min    -21.000000       0.000000       1.000000           1.000000   \n",
       "25%    132.000000    2025.000000     307.000000          14.000000   \n",
       "50%    285.000000    2560.000000     408.000000          17.000000   \n",
       "75%    438.000000    2798.000000     457.000000          20.000000   \n",
       "max    591.000000    3245.000000     550.000000         190.000000   \n",
       "\n",
       "       page_token_cnt  \n",
       "count      613.000000  \n",
       "mean       575.785073  \n",
       "std        175.963767  \n",
       "min          0.000000  \n",
       "25%        506.250000  \n",
       "50%        640.000000  \n",
       "75%        699.500000  \n",
       "max        811.250000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_text)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbfd04-1bf0-44d9-bde9-fa81c848f957",
   "metadata": {},
   "source": [
    "#Splitting pages into sentences (2 ways)\n",
    "1. W splitting \". \"\n",
    "2. NLP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5ad265-16fd-4bd0-bcd0-8fdbc57dc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 613/613 [00:02<00:00, 304.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "#Add a sentencizer pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for each_item in tqdm(all_text):\n",
    "    each_item[\"sentences\"] = list(nlp(each_item[\"text\"]).sents)\n",
    "    #spaCy dtype to string for sentences\n",
    "    each_item[\"sentences\"] = [str(sentence) for sentence in each_item[\"sentences\"]]\n",
    "    each_item[\"sentence_cnt_spacy\"] = len(each_item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19e9778-0dfd-40c3-a5d1-fd1b71433c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': 4,\n",
       " 'page_char_cnt': 2919,\n",
       " 'page_word_cnt': 464,\n",
       " 'page_sentence_cnt': 22,\n",
       " 'page_token_cnt': 729.75,\n",
       " 'text': 'But reality is not that simple. There are many database systems with different charac‐ teristics, because different applications have different requirements. There are vari‐ ous approaches to caching, several ways of building search indexes, and so on. When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone. This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications. We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics. In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems. We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters. In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application. Thinking About Data Systems We typically think of databases, queues, caches, etc. as being very different categories of tools. Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations. So why should we lump them all together under an umbrella term like data systems? Many new tools for data storage and processing have emerged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1]. For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka). The boundaries between the categories are becoming blurred. Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs. Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code. For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database. Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters). 4 | Chapter 1: Reliable, Scalable, and Maintainable Applications',\n",
       " 'sentences': ['But reality is not that simple.',\n",
       "  'There are many database systems with different charac‐ teristics, because different applications have different requirements.',\n",
       "  'There are vari‐ ous approaches to caching, several ways of building search indexes, and so on.',\n",
       "  'When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand.',\n",
       "  'And it can be hard to combine tools when you need to do something that a single tool cannot do alone.',\n",
       "  'This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications.',\n",
       "  'We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics.',\n",
       "  'In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems.',\n",
       "  'We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters.',\n",
       "  'In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application.',\n",
       "  'Thinking About Data Systems We typically think of databases, queues, caches, etc.',\n",
       "  'as being very different categories of tools.',\n",
       "  'Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations.',\n",
       "  'So why should we lump them all together under an umbrella term like data systems?',\n",
       "  'Many new tools for data storage and processing have emerged in recent years.',\n",
       "  'They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1].',\n",
       "  'For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka).',\n",
       "  'The boundaries between the categories are becoming blurred.',\n",
       "  'Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs.',\n",
       "  'Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code.',\n",
       "  'For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database.',\n",
       "  'Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters).',\n",
       "  '4 | Chapter 1: Reliable, Scalable, and Maintainable Applications'],\n",
       " 'sentence_cnt_spacy': 23}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb47a40-fe78-4404-a9ef-90b0651b49a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt</th>\n",
       "      <th>page_token_cnt</th>\n",
       "      <th>sentence_cnt_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>2303.140294</td>\n",
       "      <td>371.422512</td>\n",
       "      <td>17.084829</td>\n",
       "      <td>575.785073</td>\n",
       "      <td>16.045677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>177.102136</td>\n",
       "      <td>703.855066</td>\n",
       "      <td>118.008208</td>\n",
       "      <td>14.287294</td>\n",
       "      <td>175.963767</td>\n",
       "      <td>6.361999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>506.250000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>2560.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>2798.000000</td>\n",
       "      <td>457.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>699.500000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>591.000000</td>\n",
       "      <td>3245.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>811.250000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          page_no  page_char_cnt  page_word_cnt  page_sentence_cnt  \\\n",
       "count  613.000000     613.000000     613.000000         613.000000   \n",
       "mean   285.000000    2303.140294     371.422512          17.084829   \n",
       "std    177.102136     703.855066     118.008208          14.287294   \n",
       "min    -21.000000       0.000000       1.000000           1.000000   \n",
       "25%    132.000000    2025.000000     307.000000          14.000000   \n",
       "50%    285.000000    2560.000000     408.000000          17.000000   \n",
       "75%    438.000000    2798.000000     457.000000          20.000000   \n",
       "max    591.000000    3245.000000     550.000000         190.000000   \n",
       "\n",
       "       page_token_cnt  sentence_cnt_spacy  \n",
       "count      613.000000          613.000000  \n",
       "mean       575.785073           16.045677  \n",
       "std        175.963767            6.361999  \n",
       "min          0.000000            0.000000  \n",
       "25%        506.250000           14.000000  \n",
       "50%        640.000000           18.000000  \n",
       "75%        699.500000           20.000000  \n",
       "max        811.250000           34.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(all_text)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80f97cfc-2300-4b47-ba37-c2cd532030f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 613/613 [00:00<00:00, 301490.19it/s]\n"
     ]
    }
   ],
   "source": [
    "#split size to turn group of sentences into chunks\n",
    "split_size = 10 #research going on but you can take whatever\n",
    "\n",
    "#Function to split the sentences recursively.\n",
    "#i.e. 13 sentence = 10,10,3\n",
    "\n",
    "def split_chunks(input_list: list[str], size: int) -> list[list[str]]:\n",
    "    return [input_list[string:string+size] for string in range(0, len(input_list), size)]\n",
    "\n",
    "\n",
    "for each_item in tqdm(all_text):\n",
    "    each_item[\"sentence_chunks\"] = split_chunks(input_list = each_item[\"sentences\"], size = 10)\n",
    "    each_item[\"chunk_size\"] = len(each_item[\"sentence_chunks\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70c3257-0189-4d04-8f30-fdc16aea5a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': 4,\n",
       " 'page_char_cnt': 2919,\n",
       " 'page_word_cnt': 464,\n",
       " 'page_sentence_cnt': 22,\n",
       " 'page_token_cnt': 729.75,\n",
       " 'text': 'But reality is not that simple. There are many database systems with different charac‐ teristics, because different applications have different requirements. There are vari‐ ous approaches to caching, several ways of building search indexes, and so on. When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone. This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications. We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics. In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems. We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters. In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application. Thinking About Data Systems We typically think of databases, queues, caches, etc. as being very different categories of tools. Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations. So why should we lump them all together under an umbrella term like data systems? Many new tools for data storage and processing have emerged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1]. For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka). The boundaries between the categories are becoming blurred. Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs. Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code. For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database. Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters). 4 | Chapter 1: Reliable, Scalable, and Maintainable Applications',\n",
       " 'sentences': ['But reality is not that simple.',\n",
       "  'There are many database systems with different charac‐ teristics, because different applications have different requirements.',\n",
       "  'There are vari‐ ous approaches to caching, several ways of building search indexes, and so on.',\n",
       "  'When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand.',\n",
       "  'And it can be hard to combine tools when you need to do something that a single tool cannot do alone.',\n",
       "  'This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications.',\n",
       "  'We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics.',\n",
       "  'In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems.',\n",
       "  'We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters.',\n",
       "  'In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application.',\n",
       "  'Thinking About Data Systems We typically think of databases, queues, caches, etc.',\n",
       "  'as being very different categories of tools.',\n",
       "  'Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations.',\n",
       "  'So why should we lump them all together under an umbrella term like data systems?',\n",
       "  'Many new tools for data storage and processing have emerged in recent years.',\n",
       "  'They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1].',\n",
       "  'For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka).',\n",
       "  'The boundaries between the categories are becoming blurred.',\n",
       "  'Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs.',\n",
       "  'Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code.',\n",
       "  'For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database.',\n",
       "  'Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters).',\n",
       "  '4 | Chapter 1: Reliable, Scalable, and Maintainable Applications'],\n",
       " 'sentence_cnt_spacy': 23,\n",
       " 'sentence_chunks': [['But reality is not that simple.',\n",
       "   'There are many database systems with different charac‐ teristics, because different applications have different requirements.',\n",
       "   'There are vari‐ ous approaches to caching, several ways of building search indexes, and so on.',\n",
       "   'When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand.',\n",
       "   'And it can be hard to combine tools when you need to do something that a single tool cannot do alone.',\n",
       "   'This book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications.',\n",
       "   'We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics.',\n",
       "   'In this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems.',\n",
       "   'We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters.',\n",
       "   'In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application.'],\n",
       "  ['Thinking About Data Systems We typically think of databases, queues, caches, etc.',\n",
       "   'as being very different categories of tools.',\n",
       "   'Although a database and a message queue have some superficial similarity— both store data for some time—they have very different access patterns, which means different performance characteristics, and thus very different implementations.',\n",
       "   'So why should we lump them all together under an umbrella term like data systems?',\n",
       "   'Many new tools for data storage and processing have emerged in recent years.',\n",
       "   'They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories [1].',\n",
       "   'For example, there are datastores that are also used as mes‐ sage queues (Redis), and there are message queues with database-like durability guar‐ antees (Apache Kafka).',\n",
       "   'The boundaries between the categories are becoming blurred.',\n",
       "   'Secondly, increasingly many applications now have such demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and stor‐ age needs.',\n",
       "   'Instead, the work is broken down into tasks that can be performed effi‐ ciently on a single tool, and those different tools are stitched together using application code.'],\n",
       "  ['For example, if you have an application-managed caching layer (using Memcached or similar), or a full-text search server (such as Elasticsearch or Solr) separate from your main database, it is normally the application code’s responsibility to keep those caches and indexes in sync with the main database.',\n",
       "   'Figure 1-1 gives a glimpse of what this may look like (we will go into detail in later chapters).',\n",
       "   '4 | Chapter 1: Reliable, Scalable, and Maintainable Applications']],\n",
       " 'chunk_size': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6739c57-d5e5-405b-b6f5-3f9aa9392ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1029.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1004.38it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2998.07it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3008.11it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.96it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2995.22it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1495.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.08it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1990.65it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1957.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1991.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.91it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 664.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 668.10it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.05it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1002.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.40it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1955.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2011.66it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4010.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2014.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2997.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2995.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.49it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3001.65it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3006.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2019.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1987.82it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1995.86it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3006.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2007.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1004.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2972.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 996.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 997.46it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2957.21it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2994.51it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1005.83it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3021.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2071.77it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3007.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3021.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1002.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 665.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1981.25it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2007.80it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2009.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 995.09it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1499.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3002.37it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 997.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2975.39it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.84it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2015.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1995.39it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3022.56it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3019.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2026.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2009.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1976.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 996.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.29it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3054.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2007.80it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1499.04it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1991.60it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2024.77it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2007.32it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3008.83it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1496.90it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2012.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2017.46it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1981.25it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2995.22it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.02it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1000.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.91it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3004.52it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.02it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.05it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1995.86it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.45it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3016.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.88it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1991.12it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1974.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3036.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1987.82it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2972.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.29it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3001.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2999.50it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2996.64it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3002.37it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1986.41it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2020.86it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1995.86it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2008.29it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1991.12it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3010.99it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2076.90it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1935.98it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1987.82it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2999.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.05it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1489.28it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3002.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.22it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3005.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1979.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2034.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2998.07it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2014.07it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1014.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.80it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2994.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3005.23it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2997.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1983.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.84it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1991.60it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2993.79it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1000.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1982.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1993.02it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2018.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.44it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.93it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2995.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2013.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2991.66it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.08it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2372.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1977.51it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.29it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2990.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.84it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1989.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.40it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1988.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1986.41it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3003.80it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.29it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1994.44it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1995.86it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 666.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2991.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2010.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1934.20it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2009.73it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2999.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1997.29it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2998.79it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2002.05it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1999.67it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3000.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1996.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3008.83it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1021.51it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3985.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3986.98it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4000.29it/s]\n",
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2985.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#Making data more granular by splitting chunks into their own seperate paragraphs.\n",
    "chunk_final_collection = []\n",
    "for item in all_text:\n",
    "    for chunk in tqdm(item['sentence_chunks']):\n",
    "        chunk_dictionary = {}\n",
    "        chunk_dictionary['page_no'] = item['page_no']\n",
    "        chunk_paragraph = \" \".join(chunk)\n",
    "        fixed_text = re.sub(r'(\\w+)[\\-‐]\\s+(\\w+)', r'\\1\\2', chunk_paragraph)\n",
    "        chunk_dictionary['chunk_paragraph'] = fixed_text\n",
    "        chunk_dictionary['chunk_char_cnt'] = len(chunk_dictionary['chunk_paragraph'])\n",
    "        chunk_dictionary['chunk_token_cnt'] = len(chunk_dictionary['chunk_paragraph']) / 4\n",
    "        chunk_dictionary['chunk_word_cnt'] = len(chunk_dictionary['chunk_paragraph'].split(' '))\n",
    "        chunk_final_collection.append(chunk_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eca4bdd-497a-4c34-a7f7-b002bd835e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': 14,\n",
       " 'chunk_paragraph': 'Latency and response time Latency and response time are often used synonymously, but they are not the same. The response time is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays. Latency is the duration that a request is waiting to be handled—during which it is latent, awaiting service [17]. Even if you only make the same request over and over again, you’ll get a slightly different response time on every try. In practice, in a system handling a variety of requests, the response time can vary a lot. We therefore need to think of response time not as a single number, but as a distribution of values that you can measure. In Figure 1-4, each gray bar represents a request to a service, and its height shows how long that request took. Most requests are reasonably fast, but there are occasional outliers that take much longer. Perhaps the slow requests are intrinsically more expensive, e.g., because they process more data. But even in a scenario where you’d think all requests should take the same time, you get variation: random additional latency could be introduced by a context switch to a background process, the loss of a network packet and TCP retransmission, a garbage collection pause, a page fault forcing a read from disk, mechanical vibrations in the server rack [18], or many other causes.',\n",
       " 'chunk_char_cnt': 1395,\n",
       " 'chunk_token_cnt': 348.75,\n",
       " 'chunk_word_cnt': 238}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_final_collection[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da1d7a1c-8beb-4f46-acd2-c5dbf3baa3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>chunk_char_cnt</th>\n",
       "      <th>chunk_token_cnt</th>\n",
       "      <th>chunk_word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1239.000000</td>\n",
       "      <td>1239.000000</td>\n",
       "      <td>1239.000000</td>\n",
       "      <td>1239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>286.660210</td>\n",
       "      <td>1135.793382</td>\n",
       "      <td>283.948345</td>\n",
       "      <td>182.500404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>170.701532</td>\n",
       "      <td>534.753937</td>\n",
       "      <td>133.688484</td>\n",
       "      <td>83.523940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>139.500000</td>\n",
       "      <td>869.500000</td>\n",
       "      <td>217.375000</td>\n",
       "      <td>135.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>288.000000</td>\n",
       "      <td>1206.000000</td>\n",
       "      <td>301.500000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>433.000000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>360.750000</td>\n",
       "      <td>238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>591.000000</td>\n",
       "      <td>2900.000000</td>\n",
       "      <td>725.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           page_no  chunk_char_cnt  chunk_token_cnt  chunk_word_cnt\n",
       "count  1239.000000     1239.000000      1239.000000     1239.000000\n",
       "mean    286.660210     1135.793382       283.948345      182.500404\n",
       "std     170.701532      534.753937       133.688484       83.523940\n",
       "min     -21.000000        3.000000         0.750000        1.000000\n",
       "25%     139.500000      869.500000       217.375000      135.500000\n",
       "50%     288.000000     1206.000000       301.500000      196.000000\n",
       "75%     433.000000     1443.000000       360.750000      238.000000\n",
       "max     591.000000     2900.000000       725.000000      395.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(chunk_final_collection)\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a9523-63dc-4b40-aa38-f5589d6c026c",
   "metadata": {},
   "source": [
    "SO the max chunk token count is 725. and the model for embedding we have selected all-mpnet-base-v2, you can check the stats - https://sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "So vast majority of chunks are ~under 360 tokens which is fine as model's max sequence len is 384, but we will loose some tokens for 725 - 384 = 341 token loss.\n",
    "\n",
    "can be resolved by replacing 10 sentences per chunk to less than that.\n",
    "you can also experiment with overlaping of the sentences in chunks to get proper data together.\n",
    "Also some chunks might be useless as you can see from the min token count so we can sample it and experiment with it to filter it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeca2800-c96e-4fda-9ab8-6a77044d0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page -6 | xiv | Preface\n",
      "\n",
      "Page -5 | Preface | xv\n",
      "\n",
      "Page -1 | You’re the best. Preface | xix\n",
      "\n",
      "Page 62 | 62 | Chapter 2: Data Models and Query Languages\n",
      "\n",
      "Page 64 | 64 | Chapter 2: Data Models and Query Languages\n",
      "\n",
      "Page 66 | 66 | Chapter 2: Data Models and Query Languages\n",
      "\n",
      "Page 93 | Transaction Processing or Analytics? | 93\n",
      "\n",
      "Page 103 | Summary | 103\n",
      "\n",
      "Page 114 | 114 | Chapter 4: Encoding and Evolution\n",
      "\n",
      "Page 118 | Figure 4-2. Example record encoded using Thrift’s BinaryProtocol. 118 | Chapter 4: Encoding and Evolution\n",
      "\n",
      "Page 141 | Morgan Kaufmann, 1999. ISBN: 978-0-122-33435-1 Summary | 141\n",
      "\n",
      "Page 143 | 54] Fred Hebert: “Postscript: Maps,” learnyousomeerlang.com, April 9, 2014. Summary | 143\n",
      "\n",
      "Page 151 | 151\n",
      "\n",
      "Page 162 | 162 | Chapter 5: Replication\n",
      "\n",
      "Page 174 | Even if the application checks availability before 174 | Chapter 5: Replication\n",
      "\n",
      "Page 179 | Leaderless Replication | 179\n",
      "\n",
      "Page 181 | Leaderless Replication | 181\n",
      "\n",
      "Page 195 | sdf.org, January 4, 2016. Summary | 195\n",
      "\n",
      "Page 196 | 196 | Chapter 5: Replication\n",
      "\n",
      "Page 215 | Request Routing | 215\n",
      "\n",
      "Page 235 | Weak Isolation Levels | 235\n",
      "\n",
      "Page 240 | By carefully defining visibility rules, 240 | Chapter 7: Transactions\n",
      "\n",
      "Page 242 | 242 | Chapter 7: Transactions\n",
      "\n",
      "Page 249 | Weak Isolation Levels | 249\n",
      "\n",
      "Page 251 | Serializability | 251\n",
      "\n",
      "Page 253 | Serializability | 253\n",
      "\n",
      "Page 254 | 254 | Chapter 7: Transactions\n",
      "\n",
      "Page 266 | It is usually implemented with multi-version concurrency control (MVCC). 266 | Chapter 7: Transactions\n",
      "\n",
      "Page 267 | Summary | 267\n",
      "\n",
      "Page 273 | 273\n",
      "\n",
      "Page 297 | Unreliable Clocks | 297\n",
      "\n",
      "Page 300 | 300 | Chapter 8: The Trouble with Distributed Systems\n",
      "\n",
      "Page 314 | 314 | Chapter 8: The Trouble with Distributed Systems\n",
      "\n",
      "Page 335 | Figure 9-7. A network interruption forcing a choice between linearizability and availability. Linearizability | 335\n",
      "\n",
      "Page 343 | Ordering Guarantees | 343\n",
      "\n",
      "Page 345 | Two Ordering Guarantees | 345\n",
      "\n",
      "Page 351 | The same is not the case Ordering Guarantees | 351\n",
      "\n",
      "Page 358 | 358 | Chapter 9: Consistency and Consensus\n",
      "\n",
      "Page 365 | In particular, 2PC does not meet the requirements for termination. Distributed Transactions and Consensus | 365\n",
      "\n",
      "Page 367 | Distributed Transactions and Consensus | 367\n",
      "\n",
      "Page 368 | 368 | Chapter 9: Consistency and Consensus\n",
      "\n",
      "Page 372 | 372 | Chapter 9: Consistency and Consensus\n",
      "\n",
      "Page 397 | The output MapReduce and Distributed Filesystems | 397\n",
      "\n",
      "Page 418 | 418 | Chapter 10: Batch Processing\n",
      "\n",
      "Page 431 | Summary | 431\n",
      "\n",
      "Page 434 | ISBN: 978-1-449-35904-1 434 | Chapter 10: Batch Processing\n",
      "\n",
      "Page 442 | See also “TCP Versus UDP” on page 283.) 442 | Chapter 11: Stream Processing\n",
      "\n",
      "Page 447 | Transmitting Event Streams | 447\n",
      "\n",
      "Page 466 | Hosted services include Google Cloud Dataflow and Azure Stream Analytics. 466 | Chapter 11: Stream Processing\n",
      "\n",
      "Page 484 | 484 | Chapter 11: Stream Processing\n",
      "\n",
      "Page 523 | The Aiming for Correctness | 523\n",
      "\n",
      "Page 526 | In these cases, the constraint of “one person per seat” is delib526 | Chapter 12: The Future of Data Systems\n",
      "\n",
      "Page 529 | If the application uses the database incorrectly Aiming for Correctness | 529\n",
      "\n",
      "Page 542 | Updated regulations are now being developed [89]. 542 | Chapter 12: The Future of Data Systems\n",
      "\n",
      "Page 551 | Summary | 551\n",
      "\n",
      "Page 556 | replication Keeping a copy of the same data on several nodes (replicas) so that it remains lock 556 | Glossary\n",
      "\n",
      "Page 558 | transaction 558 | Glossary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = df3[df3['chunk_token_cnt'] < 30]\n",
    "for idx, row in filtered.iterrows():\n",
    "    print(f\"Page {row['page_no']} | {row['chunk_paragraph']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103b491b-7ae9-40eb-aa00-6b52acea31dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': -19,\n",
       "  'chunk_paragraph': 'Martin Kleppmann Designing Data-Intensive Applications The Big Ideas Behind Reliable, Scalable, and Maintainable Systems Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing',\n",
       "  'chunk_char_cnt': 200,\n",
       "  'chunk_token_cnt': 50.0,\n",
       "  'chunk_word_cnt': 24},\n",
       " {'page_no': -18,\n",
       "  'chunk_paragraph': '978-1-449-37332-0 [LSI] Designing Data-Intensive Applications by Martin Kleppmann Copyright © 2017 Martin Kleppmann. All rights reserved. Printed in the United States of America. Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. Editors: Ann Spencer and Marie Beaugureau Indexer: Ellen Troutman-Zaig Production Editor: Kristen Brown Interior Designer: David Futato Copyeditor: Rachel Head Cover Designer: Karen Montgomery Proofreader: Amanda Kersey Illustrator: Rebecca Demarest March 2017: First Edition Revision History for the First Edition 2017-03-01: First Release See http://oreilly.com/catalog/errata.csp?isbn=9781449373320 for release details. The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing Data-Intensive Applications, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk.',\n",
       "  'chunk_char_cnt': 1580,\n",
       "  'chunk_token_cnt': 395.0,\n",
       "  'chunk_word_cnt': 209},\n",
       " {'page_no': -18,\n",
       "  'chunk_paragraph': 'If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.',\n",
       "  'chunk_char_cnt': 254,\n",
       "  'chunk_token_cnt': 63.5,\n",
       "  'chunk_word_cnt': 41}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = df3[df3['chunk_token_cnt'] > 30]\n",
    "chunk_filtered_collection = filtered.to_dict(orient='records')\n",
    "chunk_filtered_collection[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a0445e8-dd28-4d79-8774-0ed9c8985e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "embedding_model = SentenceTransformer(model_name_or_path = 'all-mpnet-base-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b815588d-534a-49c5-ac54-40f84700579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1180/1180 [01:59<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14min 7s\n",
      "Wall time: 1min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Doing one by one (can use cpu if you dont want to use gpu )\n",
    "embedding_model.to('cuda')\n",
    "for item in tqdm(chunk_filtered_collection):\n",
    "    item['embedding'] = embedding_model.encode(item['chunk_paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351acdca-a18c-4f90-ae77-bcd6dd37c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.3.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83195414-4b71-49f2-94e6-06142771aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f91599db-c081-481f-9bc2-c212638e76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to do this in batches to check the performance\n",
    "\n",
    "chunk_paragraph_batch = [item['chunk_paragraph'] for item in chunk_filtered_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea1cde80-ac3a-4149-93c7-ae33c9661616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 53s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170,  0.0346, -0.0160,  ..., -0.0053,  0.0476, -0.0381],\n",
       "        [ 0.0187,  0.0161, -0.0547,  ...,  0.0029,  0.0111, -0.0091],\n",
       "        [-0.0161,  0.0439, -0.0165,  ..., -0.0552, -0.0096,  0.0129],\n",
       "        ...,\n",
       "        [ 0.0387,  0.0327, -0.0413,  ...,  0.0312,  0.0661, -0.0337],\n",
       "        [ 0.0312,  0.0312, -0.0283,  ...,  0.0445, -0.0362, -0.0305],\n",
       "        [ 0.0362,  0.0349, -0.0226,  ..., -0.0085,  0.0026, -0.0245]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embedding_model.to('cuda')\n",
    "text_chunk_embeddings = embedding_model.encode(chunk_paragraph_batch, batch_size = 10, convert_to_tensor = True)\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7db08-87db-444d-86fe-d1f6092b5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save embeddings to a file\n",
    "#chunk_filtered_collection[34]\n",
    "embeddings_df = pd.DataFrame(chunk_filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4ffb1-22b5-4ac4-a7ff-9251c81afbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.to_csv('embeddings_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201ec2c-92f4-4f73-ade9-f8168e280d9f",
   "metadata": {},
   "source": [
    "## Second part is to do RAG Search and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dfd64-5338-4aac-9d71-240bef4192f1",
   "metadata": {},
   "source": [
    "Goal - retrieve relevent data absed on query and use that data to augment input to LLM so it can generate the output based on that data\\\n",
    "Comparing embeddings - similarity search, vector search, semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2789b2b-d27f-4824-972a-e0b76d2c2a90",
   "metadata": {},
   "source": [
    "I have cloded the notebook so need to create embedding model again ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14596047-9f91-4a61-a00b-43a44d9ef87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1180, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "embedding_model = SentenceTransformer(model_name_or_path = 'all-mpnet-base-v2', device = device)\n",
    "\n",
    "#If reading from csv you will have to convert the embeddings to np.array/correct format and also need to make the dorrect datatype eg. torch.float32.\n",
    "# I had to rerun the whole thing so fine but yaad rakh chutiye\n",
    "text_chunk_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceaf451-ccdc-4c40-a04d-d258c04a653d",
   "metadata": {},
   "source": [
    "Embedding model is ready, now need to build a semantic search pipeline\\\n",
    "essentially query a text and get back passage/paragraph from textbook that is relevant.\n",
    "\n",
    "steps:\n",
    "\n",
    "1. give a query string\n",
    "2. convert query string into embedding (using the same embedding model and device we used for the pdf data - all-mpnet-base-v2, cuda)\n",
    "3. perform dot product or cosine similarity to get top k matches with ranks (we will call ranks based on similarity score we get. that should be a number the higher the better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d53a4d94-9d58-4bb1-8d1a-89348ec21a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 1180 embeddings: 0.00168 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6441, 0.6224, 0.6221, 0.6095, 0.5983, 0.5607, 0.5604, 0.5448, 0.5443,\n",
       "        0.5302], device='cuda:0'),\n",
       "indices=tensor([ 810,   20,  311,  809,    7,  398, 1168,  445,  313,  783],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a query\n",
    "query = 'distributed file system'\n",
    "\n",
    "#we need to embed the query\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor= True).to(\"cuda\")\n",
    "\n",
    "#we are using dot product\n",
    "from time import perf_counter as timer\n",
    "strat_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=text_chunk_embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Time taken to get scores on {len(text_chunk_embeddings)} embeddings: {end_time - strat_time:.5f} seconds.\")\n",
    "\n",
    "#we want the top 10 results\n",
    "\n",
    "top_results = torch.topk(dot_scores, k=10)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cebc297-0623-4372-a620-c215fc124209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': 398,\n",
       " 'chunk_paragraph': 'Shared-disk storage is implemented by a centralized storage appliance, often using custom hardware and special network infrastructure such as Fibre Channel. On the other hand, the shared-nothing approach requires no special hardware, only computers connected by a conventional datacenter network. HDFS consists of a daemon process running on each machine, exposing a network service that allows other nodes to access files stored on that machine (assuming that every general-purpose machine in a datacenter has some disks attached to it). A central server called the NameNode keeps track of which file blocks are stored on which machine. Thus, HDFS conceptually creates one big filesystem that can use the space on the disks of all machines running the daemon. In order to tolerate machine and disk failures, file blocks are replicated on multiple machines. Replication may mean simply several copies of the same data on multiple machines, as in Chapter 5, or an erasure coding scheme such as Reed–Solomon codes, which allows lost data to be recovered with lower storage overhead than full replication [20, 22]. The techniques are similar to RAID, which provides redundancy across several disks attached to the same machine; the difference is that in a distributed filesystem, file access and replication are done over a conventional datacenter network without special hardware. 398 | Chapter 10: Batch Processing',\n",
       " 'chunk_char_cnt': 1413,\n",
       " 'chunk_token_cnt': 353.25,\n",
       " 'chunk_word_cnt': 219,\n",
       " 'embedding': array([ 3.79256196e-02, -6.26387224e-02, -7.63237418e-04,  4.72893659e-03,\n",
       "        -6.18551522e-02,  1.62961334e-02,  6.56889528e-02,  1.97559763e-02,\n",
       "        -8.42817035e-03,  1.52718788e-02,  3.26216221e-02,  4.39967494e-03,\n",
       "         4.31393599e-03, -1.61065292e-02, -1.84227675e-02, -1.99044738e-02,\n",
       "         6.07561767e-02,  6.32444397e-02, -3.84967290e-02, -6.48490712e-02,\n",
       "         1.09296199e-02, -1.33062322e-02,  1.62678938e-02, -7.97347073e-03,\n",
       "        -1.91634707e-02, -6.97027221e-02, -3.60165797e-02,  4.90212813e-02,\n",
       "        -4.03929651e-02,  2.05395874e-02,  1.07018892e-02,  4.56100740e-02,\n",
       "        -2.23656707e-02,  5.71963452e-02,  1.79614995e-06, -2.39690747e-02,\n",
       "        -4.29164432e-02,  8.34355038e-03, -3.61044295e-02,  1.62784755e-02,\n",
       "         1.68617461e-02,  3.33844014e-02, -3.85120064e-02,  5.45326527e-03,\n",
       "         4.05767858e-02, -4.69906628e-02,  4.37793620e-02,  2.10400652e-02,\n",
       "        -9.63468454e-04, -7.03123882e-02, -2.49518058e-03, -2.52385270e-02,\n",
       "        -4.13916744e-02,  3.08429748e-02, -2.59651174e-03,  5.55926673e-02,\n",
       "         5.86441299e-03,  8.87265354e-02,  5.68903796e-02, -1.01591693e-02,\n",
       "        -3.08029242e-02, -3.00282566e-03,  2.18508374e-02,  1.75733585e-02,\n",
       "         5.46900742e-02,  2.97039635e-02, -2.55493745e-02, -3.14304605e-02,\n",
       "        -1.01913661e-02, -2.33213343e-02,  3.85526866e-02, -5.60159683e-02,\n",
       "         1.94810331e-02,  2.54272167e-02, -8.53862911e-02, -1.69844506e-03,\n",
       "        -1.76455476e-03,  4.87591177e-02, -8.74122139e-03,  1.35172307e-02,\n",
       "        -5.33986203e-02,  4.26274799e-02,  5.07567003e-02, -2.60469820e-02,\n",
       "         3.88169885e-02,  5.56360814e-04, -1.33253532e-02, -1.20387347e-02,\n",
       "        -1.82439331e-02,  4.19854708e-02,  1.26548875e-02, -4.27343212e-02,\n",
       "         8.52442086e-02, -4.48312378e-03,  5.35141155e-02, -1.92782171e-02,\n",
       "        -5.17649204e-02, -1.95336081e-02,  6.55632839e-02, -3.34227905e-02,\n",
       "        -9.47266445e-03, -1.74730811e-02, -1.86255686e-02,  2.31290259e-03,\n",
       "         3.66661064e-02, -1.54132806e-02, -4.33338173e-02, -4.83656675e-02,\n",
       "         2.34160945e-02, -1.78949349e-02, -1.08425124e-02, -4.92647216e-02,\n",
       "         1.66441519e-02,  6.33777380e-02,  1.40911974e-02, -4.43433970e-02,\n",
       "         2.28419434e-02,  2.49514543e-02,  1.92706883e-02,  3.50177549e-02,\n",
       "         2.46918318e-03, -1.43342083e-02, -5.46600320e-04, -2.66010799e-02,\n",
       "         8.09430983e-03,  1.73676163e-02, -4.77021039e-02, -5.11119589e-02,\n",
       "         3.66175920e-02, -7.67057575e-03,  3.73677351e-03, -4.01945673e-02,\n",
       "         6.37892559e-02,  2.80065760e-02,  2.84411721e-02, -1.52074732e-02,\n",
       "         2.20039841e-02,  3.10618244e-02, -1.14509221e-02,  3.11462139e-03,\n",
       "         5.17620742e-02, -6.02558665e-02, -6.63531646e-02, -2.84565915e-03,\n",
       "        -2.79830606e-03,  4.44899537e-02,  5.04729636e-02,  5.61285913e-02,\n",
       "        -3.31309959e-02, -6.91818481e-04, -4.39260527e-02,  1.80073641e-02,\n",
       "        -8.28754604e-02, -1.65021177e-02,  2.61051189e-02,  5.37120961e-02,\n",
       "        -1.18137272e-02, -8.77634287e-02, -5.89926094e-02, -3.78714800e-02,\n",
       "         3.69807240e-03,  1.14645995e-02,  1.15389787e-02, -8.95103440e-03,\n",
       "        -4.02565710e-02, -2.13694270e-03, -2.84489039e-02,  4.96844836e-02,\n",
       "        -2.14709733e-02, -9.53057408e-03, -3.62848304e-02,  5.84186912e-02,\n",
       "        -2.85545178e-03, -2.68190298e-02,  2.34185951e-03,  7.55269900e-02,\n",
       "         2.91816536e-02,  4.17306609e-02, -1.05306553e-02,  6.34554252e-02,\n",
       "         7.24055991e-03,  8.12820811e-03,  2.02367529e-02,  3.25499661e-02,\n",
       "         5.40441461e-02, -1.27158007e-02,  5.72568402e-02, -3.97445895e-02,\n",
       "        -1.17208688e-02,  1.62630044e-02, -2.90119294e-02,  2.98106074e-02,\n",
       "        -2.93677449e-02, -3.82193588e-02, -2.34211124e-02,  4.54115272e-02,\n",
       "         5.18315053e-03,  2.58876327e-02, -3.62033746e-03, -2.54023504e-02,\n",
       "         3.90561782e-02, -1.24150701e-03,  4.29084450e-02,  2.54086275e-02,\n",
       "         4.12446186e-02,  3.29267308e-02, -2.68183146e-02,  1.27508957e-03,\n",
       "        -4.97463741e-04, -7.75050446e-02,  1.86795264e-03, -4.40195482e-03,\n",
       "        -3.62440348e-02, -4.94819619e-02,  4.54323366e-03,  6.98312148e-02,\n",
       "         3.65631096e-02,  1.19780237e-03, -5.85395209e-02,  1.19517073e-02,\n",
       "        -2.70410571e-02,  4.71955352e-02, -4.43371497e-02, -7.74435997e-02,\n",
       "         2.16021780e-02,  4.54061404e-02, -4.08746600e-02,  7.78840622e-03,\n",
       "        -3.27849835e-02, -1.67247597e-02, -3.16120759e-02, -1.86009984e-02,\n",
       "         5.08429855e-02,  7.05850869e-02, -2.77406424e-02, -7.18908338e-03,\n",
       "         4.69247326e-02,  7.43670911e-02, -2.59200707e-02,  1.17093269e-02,\n",
       "         4.54952829e-02,  2.18932852e-02,  2.51782015e-02,  3.00169969e-03,\n",
       "        -7.43021443e-03, -3.20910774e-02, -3.64587344e-02, -2.09372342e-02,\n",
       "         2.86310948e-02, -8.21920671e-03, -2.28511281e-02,  3.17424536e-02,\n",
       "         2.35518464e-03,  3.87028083e-02,  2.87788883e-02, -3.90965827e-02,\n",
       "         5.65620959e-02,  8.93616304e-02, -1.96623392e-02,  1.99787561e-02,\n",
       "         3.26980650e-02, -7.87207931e-02,  2.25365143e-02,  1.59567446e-02,\n",
       "        -4.36137281e-02, -3.15192826e-02,  1.96944829e-02,  2.17270590e-02,\n",
       "         4.62537855e-02,  1.67892291e-03,  2.78799422e-02,  3.14906016e-02,\n",
       "         3.44341770e-02,  2.08927114e-02, -3.43917422e-02,  1.49374353e-02,\n",
       "         3.76437269e-02,  2.63108928e-02,  2.05822219e-03,  3.28528062e-02,\n",
       "         7.38711748e-03, -4.64304127e-02,  6.69931322e-02, -3.05569805e-02,\n",
       "         4.86495346e-02,  2.60564871e-02,  3.44044864e-02,  3.19874510e-02,\n",
       "        -5.76613843e-02,  3.42865586e-02, -5.35061024e-02, -2.95178425e-02,\n",
       "         2.41378471e-02, -2.60053715e-03,  3.22210118e-02, -6.49063885e-02,\n",
       "        -9.56153497e-03,  5.84602691e-02, -6.74905032e-02,  2.06315462e-02,\n",
       "        -5.82467653e-02,  2.92113163e-02,  8.85454006e-04, -8.68093781e-03,\n",
       "         1.65532269e-02, -1.10431779e-02, -1.36840502e-02,  2.33953465e-02,\n",
       "        -3.03481054e-03, -8.01665522e-03,  3.09346579e-02, -4.78080139e-02,\n",
       "        -3.51266414e-02, -3.96444350e-02, -6.65305182e-03, -1.80054596e-03,\n",
       "        -4.03907560e-02, -1.77779216e-02, -2.39248592e-02, -4.91603278e-03,\n",
       "        -1.70216300e-02, -4.21737172e-02,  2.90120416e-03, -2.71734539e-02,\n",
       "        -5.02207801e-02,  3.09117418e-02, -7.14472607e-02, -3.15894629e-03,\n",
       "         4.57210504e-02, -8.47564545e-03,  3.58686782e-02, -2.88061122e-03,\n",
       "        -6.32825717e-02,  6.80956468e-02,  3.34989689e-02,  8.44721275e-04,\n",
       "        -4.50862050e-02, -1.13650583e-01, -1.97282596e-03, -3.13857906e-02,\n",
       "         1.81485713e-02,  1.89192165e-02,  2.37590764e-02, -6.77036960e-03,\n",
       "         1.51281254e-02, -4.23576795e-02, -5.70311472e-02,  1.45840188e-02,\n",
       "        -1.69437118e-02,  1.97921717e-03,  6.54394552e-02,  3.73720117e-02,\n",
       "        -1.02997171e-02,  1.03099588e-02, -1.04696658e-02, -7.77043495e-03,\n",
       "        -6.85999468e-02, -8.26944783e-03, -3.40580195e-02,  2.45210640e-02,\n",
       "        -1.69144769e-03,  5.21304309e-02,  2.07603816e-02, -5.09378547e-03,\n",
       "         2.99089663e-02,  3.13529447e-02,  4.70078290e-02,  1.30746169e-02,\n",
       "        -7.27761816e-03, -1.73151214e-02,  1.33487415e-02,  3.64459753e-02,\n",
       "        -3.84276398e-02,  2.52361055e-02, -6.11457117e-02, -3.25598866e-02,\n",
       "         1.49322841e-02, -4.08991762e-02,  4.83550597e-03,  2.85650697e-02,\n",
       "        -3.31896506e-02, -7.92756155e-02,  3.71207297e-02, -2.57030566e-04,\n",
       "        -9.91005450e-03, -3.68320122e-02,  1.47284726e-02,  1.97614338e-02,\n",
       "         8.96122213e-03,  2.58138459e-02,  2.21348666e-02, -1.53119620e-02,\n",
       "        -5.05087674e-02,  1.05302064e-02, -1.17975967e-02,  1.59527399e-02,\n",
       "        -1.15975477e-02, -1.74026527e-02,  1.01046031e-02,  3.25368121e-02,\n",
       "         3.37161534e-02,  6.01285510e-02,  4.93015498e-02,  1.10594053e-02,\n",
       "        -3.56376655e-02,  5.73345274e-03,  1.27085466e-02, -4.11113072e-03,\n",
       "         2.45252978e-02,  4.00426835e-02, -2.50326041e-02, -9.96813551e-03,\n",
       "        -1.50053145e-03, -4.77325954e-02, -4.48904410e-02,  4.21670713e-02,\n",
       "        -2.29309890e-02, -4.14007250e-03,  1.29048852e-02,  5.47217913e-02,\n",
       "        -2.03740895e-02, -4.21580933e-02,  1.12653729e-02, -4.14198115e-02,\n",
       "         1.24444906e-02,  3.44579555e-02,  1.76901985e-02,  8.17336291e-02,\n",
       "         3.65033150e-02,  1.60454623e-02,  9.71294194e-03, -1.74678490e-02,\n",
       "         1.54497307e-02,  3.30418572e-02,  3.41764688e-02,  3.05343028e-02,\n",
       "        -4.46270406e-02, -2.82123350e-02,  9.80250165e-02,  6.25382066e-02,\n",
       "        -1.93489287e-02, -8.90059397e-03,  5.00409901e-02,  2.69452576e-02,\n",
       "        -2.36124732e-03,  2.50155982e-02, -7.18553737e-03, -3.53617743e-02,\n",
       "        -6.61027208e-02, -3.09106540e-02, -2.37103142e-02,  4.88084089e-03,\n",
       "         5.03303297e-02,  1.24967843e-02,  4.05411571e-02, -1.96835864e-02,\n",
       "        -8.22116658e-02,  3.10112927e-02,  4.07196861e-03,  2.30375845e-02,\n",
       "        -3.65036860e-04,  3.27995680e-02,  7.77033530e-03,  3.94108705e-02,\n",
       "        -5.01512438e-02, -4.70594130e-02, -4.38763238e-02, -2.19815839e-02,\n",
       "         1.64934099e-02, -2.77229380e-02, -5.88320643e-02,  1.47104003e-02,\n",
       "        -2.04163194e-02,  8.62132572e-03, -3.58128658e-04,  9.73298680e-03,\n",
       "        -4.85209078e-02,  2.86852894e-03,  5.13802795e-03, -4.22676317e-02,\n",
       "         3.01744845e-02, -5.91852963e-02,  5.08582511e-04,  7.39271194e-03,\n",
       "         6.89818636e-02,  4.45501730e-02,  1.70912407e-02, -1.99734103e-02,\n",
       "        -4.27763863e-03, -2.24982016e-02,  3.45093422e-02, -2.57102419e-02,\n",
       "        -4.85245921e-02,  8.51399526e-02,  5.76195382e-02,  3.30915824e-02,\n",
       "        -1.98777788e-03, -2.95215063e-02, -1.10156946e-02,  1.60584645e-03,\n",
       "        -4.25352994e-03, -1.65940803e-02, -2.65286751e-02,  3.53487348e-03,\n",
       "         4.22147252e-02,  3.05537954e-02, -2.14857385e-02,  2.27600299e-02,\n",
       "         6.92758802e-03,  7.18411198e-03, -2.20465567e-02, -2.24040840e-02,\n",
       "        -1.77795421e-02, -1.00512709e-02,  6.78569227e-02, -4.15769368e-02,\n",
       "         8.52511451e-03, -5.13596786e-03, -2.81587876e-02, -2.51654740e-02,\n",
       "         2.03766543e-02, -3.81729640e-02, -6.97380491e-03, -8.15256387e-02,\n",
       "         2.00820863e-02, -7.66371265e-02, -3.88736697e-03, -2.69884840e-02,\n",
       "         1.96651071e-02, -3.74511629e-02,  6.39792532e-03, -3.44386734e-02,\n",
       "        -7.43006393e-02, -1.39583498e-02,  1.63956899e-02,  3.01800258e-02,\n",
       "         4.57464010e-02,  3.43243852e-02, -6.08133851e-03, -6.31163791e-02,\n",
       "         2.35299990e-02, -6.27945503e-03,  2.05225162e-02,  2.91746687e-02,\n",
       "         8.63208696e-02, -7.29280934e-02,  1.19593064e-03,  1.48050841e-02,\n",
       "         4.00140807e-02, -1.64332800e-02,  1.35278674e-02,  7.68410340e-02,\n",
       "        -2.31779255e-02, -5.52698690e-03,  1.18740893e-03, -5.45632372e-33,\n",
       "         7.16674998e-02, -5.80586493e-02, -1.21618051e-03,  1.28098764e-03,\n",
       "        -5.79028130e-02,  2.91764010e-02, -4.61132377e-02, -5.16603142e-02,\n",
       "         2.82746889e-02,  6.72934530e-03,  1.68382768e-02,  3.72484252e-02,\n",
       "         3.33736613e-02,  3.70365307e-02,  2.95931543e-03, -1.78242754e-02,\n",
       "        -2.50399224e-02,  2.37722136e-02, -6.40262291e-03, -3.03591862e-02,\n",
       "         7.96865821e-02,  7.02596875e-03, -2.22208276e-02,  8.68944377e-02,\n",
       "        -7.65079958e-03, -9.90666598e-02,  7.33353896e-03, -1.14907359e-03,\n",
       "        -4.40769037e-03, -2.71868277e-02,  1.79384332e-02, -5.71170934e-02,\n",
       "        -2.19050106e-02, -8.87180865e-02,  2.75164880e-02,  5.24209999e-02,\n",
       "        -6.84891045e-02,  1.52488658e-02, -3.77164246e-03,  1.00875576e-03,\n",
       "        -1.67277213e-02,  2.56784800e-02, -9.64014325e-03,  5.84082818e-03,\n",
       "        -6.22289069e-02,  1.50287990e-02, -6.98080286e-02,  3.86940576e-02,\n",
       "        -4.56233211e-02,  3.91166471e-02, -1.45729550e-03,  1.04077710e-02,\n",
       "        -3.40169147e-02,  9.76829901e-02,  6.26932178e-03, -5.72988093e-02,\n",
       "        -9.12819803e-03,  2.90555209e-02, -6.13005459e-02,  2.42764316e-02,\n",
       "        -2.37667914e-02,  1.73164494e-02, -2.83350088e-02, -3.54660116e-02,\n",
       "        -1.80839549e-03,  2.33810581e-02,  3.09456661e-02, -4.30219211e-02,\n",
       "         8.86440463e-03,  8.04592520e-02, -5.69213070e-02,  2.14005392e-02,\n",
       "        -1.84603676e-03,  2.39602160e-02,  1.77135672e-02, -9.32138599e-03,\n",
       "        -5.30583202e-04,  9.20054689e-03, -5.11470102e-02, -7.00961798e-02,\n",
       "         1.61091425e-03,  2.86351237e-02, -4.05021124e-02, -2.66710576e-02,\n",
       "        -3.88326459e-02, -4.22208980e-02, -9.13430564e-03, -3.01721157e-03,\n",
       "        -2.49162130e-03,  3.45522203e-02, -3.02968826e-02,  1.05055787e-01,\n",
       "         9.07368492e-03, -1.25154052e-02,  1.53858913e-03,  4.50166836e-02,\n",
       "         6.22026473e-02,  1.79640623e-03, -2.08705533e-02, -1.42844124e-02,\n",
       "        -2.92089698e-03,  5.08493220e-04, -4.98198569e-02,  3.72198448e-02,\n",
       "        -5.65125654e-03, -9.80990706e-04,  1.09985089e-02,  3.00871748e-02,\n",
       "        -3.42406519e-02,  2.09421720e-02,  3.58257145e-02,  1.29826818e-04,\n",
       "        -5.13243936e-02, -7.01794773e-02, -1.22347195e-02,  2.11809706e-02,\n",
       "         4.49213982e-02,  7.73915462e-03, -2.81696431e-02, -5.67498198e-03,\n",
       "         3.18567120e-02,  2.79404242e-02, -6.09625466e-02, -1.95344933e-03,\n",
       "         3.41628143e-03,  2.57770307e-02, -8.83309543e-02, -7.96564203e-03,\n",
       "         8.43136683e-02, -2.43465509e-02,  1.96980517e-02,  3.00239939e-02,\n",
       "         2.70452887e-07, -2.56380644e-02,  2.59124152e-02,  3.71405743e-02,\n",
       "        -7.53714964e-02,  5.32293953e-02,  9.61691979e-03, -4.75682504e-02,\n",
       "         3.70896123e-02,  1.22696580e-02,  3.12369969e-02, -1.76530052e-02,\n",
       "        -2.75973184e-03,  3.07537206e-02, -4.38929163e-02, -3.64308767e-02,\n",
       "        -3.73326689e-02, -8.58039856e-02, -2.18307618e-02,  2.45125983e-02,\n",
       "        -4.78802808e-02,  8.19057785e-03,  1.37883211e-02,  9.73633025e-03,\n",
       "        -3.28013971e-02,  2.50342265e-02, -1.97758507e-02, -5.70202479e-03,\n",
       "         9.48754251e-02,  5.36319017e-02,  3.32338363e-03, -2.08484717e-02,\n",
       "        -3.32953930e-02,  5.47453528e-03, -1.39126154e-02, -3.05168871e-02,\n",
       "        -3.57132815e-02,  6.65533589e-03,  6.75162673e-02,  3.07258666e-02,\n",
       "         7.61551317e-03, -7.41845556e-03, -4.91853058e-02, -4.37327996e-02,\n",
       "         3.90224555e-03,  2.86794659e-02,  9.56522301e-03, -4.73516136e-02,\n",
       "         7.68693835e-02,  1.27651738e-02, -5.12692286e-03,  2.50215922e-02,\n",
       "        -9.36735980e-03,  1.24269295e-02,  5.98273519e-03, -4.22865301e-02,\n",
       "        -8.57549720e-03,  5.29037183e-03,  2.68492773e-02,  8.21424834e-03,\n",
       "        -4.39891964e-03, -2.53435560e-02, -2.66632140e-02, -3.47740129e-02,\n",
       "        -3.69720198e-02, -1.63163990e-02, -1.89624249e-03, -4.57404628e-02,\n",
       "         1.92993076e-34, -3.10045369e-02,  1.08190672e-02,  1.93282869e-02,\n",
       "        -2.69540716e-02, -3.65976021e-02, -4.19203080e-02,  1.75294820e-02,\n",
       "         1.66817345e-02,  9.95161198e-03,  1.35996856e-03,  1.68343273e-03],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have our old list of dictionary where we can match these indices to check the actual paragraph chunks from the pdfs\n",
    "chunk_filtered_collection[810]\n",
    "\n",
    "#top_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a00cf1-fc24-46be-96eb-454197116966",
   "metadata": {},
   "source": [
    "Searching over embeddings is fast but you will need to create indexes if you have huge embedding data\\\n",
    "you can use faiss library which helps in 'approximate nearest neighbor search' (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f62e3ffa-f989-4696-8d3c-66f4a44139a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:distributed file system\n",
      "\n",
      "results:\n",
      "score:0.6440657377243042\n",
      "Text:\n",
      "Shared-disk storage is implemented by a centralized storage appliance, often using custom hardware\n",
      "and special network infrastructure such as Fibre Channel. On the other hand, the shared-nothing\n",
      "approach requires no special hardware, only computers connected by a conventional datacenter\n",
      "network. HDFS consists of a daemon process running on each machine, exposing a network service that\n",
      "allows other nodes to access files stored on that machine (assuming that every general-purpose\n",
      "machine in a datacenter has some disks attached to it). A central server called the NameNode keeps\n",
      "track of which file blocks are stored on which machine. Thus, HDFS conceptually creates one big\n",
      "filesystem that can use the space on the disks of all machines running the daemon. In order to\n",
      "tolerate machine and disk failures, file blocks are replicated on multiple machines. Replication may\n",
      "mean simply several copies of the same data on multiple machines, as in Chapter 5, or an erasure\n",
      "coding scheme such as Reed–Solomon codes, which allows lost data to be recovered with lower storage\n",
      "overhead than full replication [20, 22]. The techniques are similar to RAID, which provides\n",
      "redundancy across several disks attached to the same machine; the difference is that in a\n",
      "distributed filesystem, file access and replication are done over a conventional datacenter network\n",
      "without special hardware. 398 | Chapter 10: Batch Processing\n",
      "score:0.6224337816238403\n",
      "Text:\n",
      "2. In Part II, we move from data stored on one machine to data that is distributed across multiple\n",
      "machines. This is often necessary for scalability, but brings with it a variety of unique\n",
      "challenges. We first discuss replication (Chapter 5), partitioning/sharding (Chapter 6), and\n",
      "transactions (Chapter 7). We then go into xvi | Preface\n",
      "score:0.6221202611923218\n",
      "Text:\n",
      "Figure II-1. A database split into two partitions, with two replicas per partition. With an\n",
      "understanding of those concepts, we can discuss the difficult trade-offs that you need to make in a\n",
      "distributed system. We’ll discuss transactions in Chapter 7, as that will help you understand all\n",
      "the many things that can go wrong in a data system, and what you can do about them. We’ll conclude\n",
      "this part of the book by discussing the fundamental limitations of distributed systems in Chapters 8\n",
      "and 9. Later, in Part III of this book, we will discuss how you can take several (potentially\n",
      "distributed) datastores and integrate them into a larger system, satisfying the needs of a complex\n",
      "application. But first, let’s talk about distributed data. References [1] Ulrich Drepper: “What\n",
      "Every Programmer Should Know About Memory,” akkadia.org, November 21, 2007. [ 2] Ben Stopford:\n",
      "“Shared Nothing vs. Shared Disk Architectures: An Independent View,” benstopford.com, November 24,\n",
      "2009. [ 3] Michael Stonebraker: “The Case for Shared Nothing,” IEEE Database Engineering Bulletin,\n",
      "volume 9, number 1, pages 4–9, March 1986. [\n",
      "score:0.6095227003097534\n",
      "Text:\n",
      "iv. One difference is that with HDFS, computing tasks can be scheduled to run on the machine that\n",
      "stores a copy of a particular file, whereas object stores usually keep storage and computation\n",
      "separate. Reading from a local disk has a performance advantage if network bandwidth is a\n",
      "bottleneck. Note however that if erasure coding is used, the locality advantage is lost, because the\n",
      "data from several machines must be combined in order to reconstitute the original file [20]. files\n",
      "are written once, in a sequential fashion (not modifying any existing part of a file once it has\n",
      "been written). While Unix tools use stdin and stdout as input and output, MapReduce jobs read and\n",
      "write files on a distributed filesystem. In Hadoop’s implementation of MapReduce, that filesystem is\n",
      "called HDFS (Hadoop Distributed File System), an open source reimplementation of the Google File\n",
      "System (GFS) [19]. Various other distributed filesystems besides HDFS exist, such as GlusterFS and\n",
      "the Quantcast File System (QFS) [20]. Object storage services such as Amazon S3, Azure Blob Storage,\n",
      "and OpenStack Swift [21] are similar in many ways.iv In this chapter we will mostly use HDFS as a\n",
      "running example, but the principles apply to any distributed filesystem. HDFS is based on the\n",
      "shared-nothing principle (see the introduction to Part II), in contrast to the shared-disk approach\n",
      "of Network Attached Storage (NAS) and Storage Area Network (SAN) architectures.\n",
      "score:0.5982774496078491\n",
      "Text:\n",
      "Part II. Distributed Data 5. Replication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 Leaders and Followers 152\n",
      "Synchronous Versus Asynchronous Replication 153 Setting Up New Followers 155 Handling Node Outages\n",
      "156 Implementation of Replication Logs 158 Problems with Replication Lag 161 Reading Your Own Writes\n",
      "162 Monotonic Reads 164 Consistent Prefix Reads 165 Solutions for Replication Lag 167 Multi-Leader\n",
      "Replication 168 Use Cases for Multi-Leader Replication 168 Handling Write Conflicts 171 Multi-Leader\n",
      "Replication Topologies 175 Leaderless Replication 177 Writing to the Database When a Node Is Down\n",
      "177 Limitations of Quorum Consistency 181 Sloppy Quorums and Hinted Handoff 183 Detecting Concurrent\n",
      "Writes 184 Summary 192 6. Partitioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 Partitioning and Replication 200\n",
      "Partitioning of Key-Value Data 201 Partitioning by Key Range 202 Partitioning by Hash of Key 203\n",
      "Skewed Workloads and Relieving Hot Spots 205 Partitioning and Secondary Indexes 206 Partitioning\n",
      "Secondary Indexes by Document 206 Partitioning Secondary Indexes by Term 208 Rebalancing Partitions\n",
      "209 Strategies for Rebalancing 210 Operations: Automatic or Manual Rebalancing 213 Request Routing\n",
      "214 Parallel Query Execution 216 Summary 216 7. Transactions. . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 The Slippery\n",
      "Concept of a Transaction 222 Table of Contents | ix\n",
      "score:0.5607478618621826\n",
      "Text:\n",
      "Summary In this chapter we looked at the issue of replication. Replication can serve several\n",
      "purposes: High availability Keeping the system running, even when one machine (or several machines,\n",
      "or an entire datacenter) goes down Disconnected operation Allowing an application to continue\n",
      "working when there is a network interruption Latency Placing data geographically close to users, so\n",
      "that users can interact with it faster Scalability Being able to handle a higher volume of reads\n",
      "than a single machine could handle, by performing reads on replicas Despite being a simple\n",
      "goal—keeping a copy of the same data on several machines— replication turns out to be a remarkably\n",
      "tricky problem. It requires carefully thinking about concurrency and about all the things that can\n",
      "go wrong, and dealing with the consequences of those faults. At a minimum, we need to deal with\n",
      "unavailable nodes and network interruptions (and that’s not even considering the more insidious\n",
      "kinds of fault, such as silent data corruption due to software bugs). We discussed three main\n",
      "approaches to replication: Single-leader replication Clients send all writes to a single node (the\n",
      "leader), which sends a stream of data change events to the other replicas (followers). Reads can be\n",
      "performed on any replica, but reads from followers might be stale. Multi-leader replication Clients\n",
      "send each write to one of several leader nodes, any of which can accept writes. The leaders send\n",
      "streams of data change events to each other and to any follower nodes. Leaderless replication\n",
      "Clients send each write to several nodes, and read from several nodes in parallel in order to detect\n",
      "and correct nodes with stale data. Each approach has advantages and disadvantages.\n",
      "score:0.560429573059082\n",
      "Text:\n",
      "software errors, 8 Remote Method Invocation (Java RMI), 134 remote procedure calls (RPCs), 134-136\n",
      "(see also services) based on futures, 135 data encoding and evolution, 136 issues with, 134 using\n",
      "Avro, 126, 135 using Thrift, 135 versus message brokers, 137 repeatable reads (transaction\n",
      "isolation), 242 replicas, 152 replication, 151-193, 556 and durability, 227 chain replication, 155\n",
      "conflict resolution and, 246 consistency properties, 161-167 consistent prefix reads, 165 monotonic\n",
      "reads, 164 reading your own writes, 162 in distributed filesystems, 398 leaderless, 177-191\n",
      "detecting concurrent writes, 184-191 limitations of quorum consistency, 181-183, 334 sloppy quorums\n",
      "and hinted handoff, 183 monitoring staleness, 182 multi-leader, 168-177 across multiple datacenters,\n",
      "168, 335 handling write conflicts, 171-175 replication topologies, 175-177 partitioning and, 147,\n",
      "200 reasons for using, 145, 151 single-leader, 152-161 failover, 157 implementation of replication\n",
      "logs, 158-161 relation to consensus, 367 setting up new followers, 155 synchronous versus\n",
      "asynchronous, 153-155 state machine replication, 349, 452 using erasure coding, 398 with\n",
      "heterogeneous data systems, 453 replication logs (see logs) reprocessing data, 496, 498 (see also\n",
      "evolvability) from log-based messaging, 451 request routing, 214-216 approaches to, 214 parallel\n",
      "query execution, 216 resilient systems, 6 (see also fault tolerance) response time as performance\n",
      "metric for services, 13, 389 guarantees on, 298 latency versus, 14 mean and percentiles, 14 user\n",
      "experience, 15 responsibility and accountability, 535 REST (Representational State Transfer), 133\n",
      "(see also services) RethinkDB (database) document data model, 31 dynamic partitioning, 212 join\n",
      "support, 34, 42 key-range partitioning, 202 leader-based replication, 153 subscribing to changes,\n",
      "456 Riak (database) Bitcask storage engine, 72 CRDTs, 174, 191 dotted version vectors, 191 gossip\n",
      "protocol, 216 hash partitioning, 203-204, 211 last-write-wins conflict resolution, 186 leaderless\n",
      "replication, 177 LevelDB storage engine, 78 linearizability, lack of, 335 multi-datacenter support,\n",
      "184 preventing lost updates across replicas, 246 rebalancing, 213 search feature, 209 secondary\n",
      "indexes, 207 siblings (concurrently written values), 190 sloppy quorums, 184 ring buffers, 450\n",
      "Ripple (cryptocurrency), 532 rockets, 10, 36, 305 RocksDB (storage engine), 78 leveled compaction,\n",
      "79 rollbacks (transactions), 222 rolling upgrades, 8, 112 routing (see request routing) row-oriented\n",
      "storage, 96 row-based replication, 160 rowhammer (memory corruption), 529 RPCs (see remote procedure\n",
      "calls) 582 | Index\n",
      "score:0.5447947978973389\n",
      "Text:\n",
      "Figure 6-7. Three different ways of routing a request to the right node. This is a challenging\n",
      "problem, because it is important that all participants agree— otherwise requests would be sent to\n",
      "the wrong nodes and not handled correctly. There are protocols for achieving consensus in a\n",
      "distributed system, but they are hard to implement correctly (see Chapter 9). Many distributed data\n",
      "systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster\n",
      "metadata, as illustrated in Figure 6-8. Each node registers itself in ZooKeeper, and ZooKeeper\n",
      "maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier\n",
      "or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a\n",
      "partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so\n",
      "that it can keep its routing information up to date. Figure 6-8. Using ZooKeeper to keep track of\n",
      "assignment of partitions to nodes.\n",
      "score:0.5442606210708618\n",
      "Text:\n",
      "CHAPTER 5 Replication The major difference between a thing that might go wrong and a thing that\n",
      "cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually\n",
      "turns out to be impossible to get at or repair. — Douglas Adams, Mostly Harmless (1992) Replication\n",
      "means keeping a copy of the same data on multiple machines that are connected via a network. As\n",
      "discussed in the introduction to Part II, there are several reasons why you might want to replicate\n",
      "data: • To keep data geographically close to your users (and thus reduce latency) • To allow the\n",
      "system to continue working even if some of its parts have failed (and thus increase availability) •\n",
      "To scale out the number of machines that can serve read queries (and thus increase read throughput)\n",
      "In this chapter we will assume that your dataset is so small that each machine can hold a copy of\n",
      "the entire dataset. In Chapter 6 we will relax that assumption and discuss partitioning (sharding)\n",
      "of datasets that are too big for a single machine. In later chapters we will discuss various kinds\n",
      "of faults that can occur in a replicated data system, and how to deal with them. If the data that\n",
      "you’re replicating does not change over time, then replication is easy: you just need to copy the\n",
      "data to every node once, and you’re done. All of the difficulty in replication lies in handling\n",
      "changes to replicated data, and that’s what this chapter is about. We will discuss three popular\n",
      "algorithms for replicating changes between nodes: single-leader, multi-leader, and leaderless\n",
      "replication. Almost all distributed databases use one of these three approaches. They all have\n",
      "various pros and cons, which we will examine in detail.\n",
      "score:0.530164361000061\n",
      "Text:\n",
      "PART III Derived Data In Parts I and II of this book, we assembled from the ground up all the major\n",
      "considerations that go into a distributed database, from the layout of data on disk all the way to\n",
      "the limits of distributed consistency in the presence of faults. However, this discussion assumed\n",
      "that there was only one database in the application. In reality, data systems are often more\n",
      "complex. In a large application you often need to be able to access and process data in many\n",
      "different ways, and there is no one database that can satisfy all those different needs\n",
      "simultaneously. Applications thus commonly use a combination of several different datastores,\n",
      "indexes, caches, analytics systems, etc. and implement mechanisms for moving data from one store to\n",
      "another. In this final part of the book, we will examine the issues around integrating multiple\n",
      "different data systems, potentially with different data models and optimized for different access\n",
      "patterns, into one coherent application architecture. This aspect of system-building is often\n",
      "overlooked by vendors who claim that their product can satisfy all your needs. In reality,\n",
      "integrating disparate systems is one of the most important things that needs to be done in a\n",
      "nontrivial application.\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the output so that it is formatted well\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text(text, wrap_length=100):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "query = 'distributed file system'\n",
    "print(f\"Query:{query}\\n\")\n",
    "print(\"results:\")\n",
    "\n",
    "for score, index in zip(top_results[0], top_results[1]):\n",
    "    print(f\"score:{score}\")\n",
    "    print(\"Text:\")\n",
    "    #print(chunk_filtered_collection[index]['chunk_paragraph'])\n",
    "    wrap_text(chunk_filtered_collection[index]['chunk_paragraph'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbf55d-f808-4260-9a25-4b56c18bfbd2",
   "metadata": {},
   "source": [
    "The results can be potentially improved by using a reranking model.\\\n",
    "(a model that is trained to rank the results eg. the 10 that we got and rank them based on the usefullness)\n",
    "\n",
    "can use mixedbreadai reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "373149ee-62e3-4244-8897-cab5999162c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 1180 embeddings: 0.00168 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6388, 0.5393, 0.5257, 0.5212, 0.5125, 0.5036, 0.5029, 0.4994, 0.4972,\n",
       "         0.4943], device='cuda:0'),\n",
       " tensor([ 880,  235, 1162,  811,  844,  215,  884,  237,  852,  847],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making a function based on the code above\n",
    "\n",
    "def retrieve_data(query: str, embeddings: torch.tensor, model: SentenceTransformer=embedding_model, k_resources_return: int=10, print_time: bool = True):\n",
    "\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor= True)\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time - strat_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, k=k_resources_return)\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "\n",
    "retrieve_data(query='hive database for big data', embeddings = text_chunk_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "952aea70-d5ba-4894-a719-1b3e166a3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local GPU memory: 8GB\n"
     ]
    }
   ],
   "source": [
    "#Checking the local VRAM\n",
    "\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes/(2**30))\n",
    "print(f\"Local GPU memory: {gpu_memory_gb}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a887ad0-1098-4b66-a724-7bac15bb8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  4 14:37:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.41                 Driver Version: 566.41         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   69C    P8             23W /   35W |     701MiB /   8192MiB |     23%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6844    C+G   ...4.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A      7176    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      8292    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10796    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     12592    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12600    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14196    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     16072    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     18068    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     18400    C+G   ...5.9.2.0_x64__htrsf667h5kn2\\AWCC.exe      N/A      |\n",
      "|    0   N/A  N/A     23492    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     25828    C+G   ...on\\135.0.3179.98\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd8e8-8338-4048-a99d-61c87de7c494",
   "metadata": {},
   "source": [
    "Loading LLM locally:\n",
    "1. Selecting an appropriate model based on your VRAM\n",
    "2. Quantization config (what precision to load your model - 16bit, 8bit, 4bit etc)\n",
    "3. Model ID - for telling the transformer what model/tokenizer to load\n",
    "4. Tokenizer - turns text to numbers ready for LLM (different than embedding model)\n",
    "5. LLM Model - for generation of the text we want\n",
    "\n",
    "there are also different ways to speed up the model eg the token generation using \"flash attention 2\" which uses the attention module in transformers to speed things up (will need compute power of 8.0+ so can check on the nvidia website for GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99e01b61-2a1d-4991-a88d-306d70fae204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability(0) #0 is for which device I only have 1 device so 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2d92f-444a-4c52-a3e5-f959a412c86b",
   "metadata": {},
   "source": [
    "We will need to download the LLM model using HuggingFace website. The model selection needs to be for HF transformer\\\n",
    "Also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146ccbb4-0a9d-447e-acf9-572b6ea0ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashb\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.30s/it]\n",
      "C:\\Users\\yashb\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yashb\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "access_token = ''  # Replace with your actual token\n",
    "#get this by creating HF account and find tokens in your profile\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f206f7-3892-4c37-b1ad-4e21f71aee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./llama-3-3b-instruct\"\n",
    "\n",
    "# Save locally\n",
    "tokenizer.save_pretrained(local_path)\n",
    "model.save_pretrained(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "754f98f7-40b2-4f83-9fa8-681c59d8edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "local_path = \"./llama-3-3b-instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.float16)\n",
    "#need to pass this as a parameter in AutoModelForCausalLM while loading the model but for now we do not need to use this\n",
    "#Also models come with bfloat16 dtype but bfloat is not working for my gpu so will need to convert it to float16\n",
    "use_quantization_config = True\n",
    "\n",
    "#Implementing flast attention 2 (which is now compatible with pytorch)\n",
    "\n",
    "if is_flash_attn_2_available() and torch.cuda.get_device_capability(0)[0] >=8:\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "    attn_implementation = 'sdpa'\n",
    "\n",
    "#Pick the model we want to use\n",
    "#model_id = model_id  \n",
    "model_id = local_path\n",
    "#model_id = google/gemma-7b-it or whatever. I am using gemma-7b-quant which I downloaded already and is quantized so no need for quantization here.\n",
    "\n",
    "#Instentiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_id)\n",
    "\n",
    "#Instantiate the LLM Model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = model_id,\n",
    "    torch_dtype = torch.float16,\n",
    "    quantization_config = quantization_config if use_quantization_config else None,\n",
    "    low_cpu_mem_usage = False,\n",
    "    attn_implementation = attn_implementation\n",
    ")\n",
    "\n",
    "if not use_quantization_config:\n",
    "    llm_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecddb8f8-dbee-4ec3-8e81-ed339c3aae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  3 21:35:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.41                 Driver Version: 566.41         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   67C    P8             23W /   35W |     321MiB /   8192MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     10860    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10884    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     11744    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12860    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12884    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14612    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     15524    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     16276    C+G   ...trsf667h5kn2\\DU\\AlienwareUpdate.exe      N/A      |\n",
      "|    0   N/A  N/A     18172    C+G   ...4.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     18428    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22308    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     22600    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     23548    C+G   ...on\\135.0.3179.98\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     25312    C+G   ...5.9.2.0_x64__htrsf667h5kn2\\AWCC.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35892e48-4969-4a70-adcb-ab52f17440e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a049958-d8ba-4a1c-ba64-335dda1392bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention 2 available: False\n",
      "CUDA Capability: (8, 6)\n",
      "Using PyTorch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(\"Flash Attention 2 available:\", is_flash_attn_2_available())\n",
    "print(\"CUDA Capability:\", torch.cuda.get_device_capability(0))\n",
    "print(\"Using PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2735649f-0cc2-4f53-b648-991d0704878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "     ---------------------------------------- 0.0/6.0 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 1.8/6.0 MB 11.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 4.5/6.0 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.0/6.0 MB 10.8 MB/s eta 0:00:00\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yashb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-re2olaza\\\\flash-attn_af51ca497c7d4e569bd29601f4df8da0\\\\csrc\\\\composable_kernel\\\\library\\\\include\\\\ck\\\\library\\\\tensor_operation_instance\\\\gpu\\\\grouped_conv_bwd_weight\\\\device_grouped_conv_bwd_weight_two_stage_xdl_instance.hpp'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install flash-attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd45058-748e-4993-a83f-12b9b596f05c",
   "metadata": {},
   "source": [
    "There is some issue with the flash attn so check this one later and still using the default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0ca75-f47d-46a9-b931-a39317c59f41",
   "metadata": {},
   "source": [
    "## Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57174136-e0a9-4146-95ec-59ff08c9aafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1803463680"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7afbb5-7035-49cd-92f1-3997227f8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "Yes, that's correct! Paris is indeed the capital of France. Well done!\n",
      "\n",
      "(Note: This question is often used as a way to test the user's knowledge of geography, particularly in relation to France. The question itself is simple, but it requires the user to recall and apply their knowledge of France's capital city.) \n",
      "\n",
      "**Explanation:** \n",
      "The question is asking for the capital city of France. The correct answer is Paris, which is a well-known fact in many parts of the world. The question is designed to be straightforward and easy to answer, making it a good choice for a multiple-choice question.\n",
      "\n",
      "**Correct answer:** Paris\n",
      "\n",
      "**Incorrect answers:** London, Berlin, Rome, etc.\n",
      "\n",
      "**Explanation:** These cities are all capitals of other countries, but not France. The incorrect answers are designed to be plausible, but not accurate, making it easy for the user to make a mistake. \n",
      "\n",
      "This type of question is often used in assessments, quizzes, and tests to evaluate the user's knowledge of geography and their ability to recall and apply information. \n",
      "\n",
      "**Tips for the user:**\n",
      "- Make sure to recall the correct capital city of France.\n",
      "- Check the options carefully to avoid making a mistake.\n",
      "- If you\n"
     ]
    }
   ],
   "source": [
    "# Set model to eval mode\n",
    "llm_model.eval()\n",
    "\n",
    "# Function to ask questions\n",
    "def ask_question(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example\n",
    "question = \"What is the capital of France?\"\n",
    "response = ask_question(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d16b9f61-d191-456b-9d95-5786c5f33037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whats the distance between India and USA? 1000 miles?\n",
      "No, not quite. The distance between India and USA is approximately 9,100 miles (14,500 km). To put it in perspective, it's roughly 14 times the distance between New York City and Los Angeles. India is a large country, and the United States is a vast continent. The distance between them is significant, making air travel a relatively long journey.\n",
      "\n",
      "Distance between India and USA: 9,100 miles (14,500 km)\n",
      "\n",
      "Note: The distance may vary depending on the specific locations within India and USA. This value is an approximate average distance between the two countries.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "question = \"Whats the distance between India and USA?\"\n",
    "response = ask_question(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099dc6c5-eb9d-426c-8f78-2a5c1d19a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the actual prompt formatting of Gemma, Llama and Mistral etc\n",
    "#Still just playing around\n",
    "def format_prompt(system_message, user_input, history=[]):\n",
    "    prompt = \"<|begin_of_text|>\"\n",
    "\n",
    "    if system_message:\n",
    "        prompt += f\"<|start_header_id|>system<|end_header_id|>\\n{system_message}<|eot_id|>\\n\"\n",
    "\n",
    "    for turn in history:\n",
    "        prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{turn['user']}<|eot_id|>\\n\"\n",
    "        prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{turn['assistant']}<|eot_id|>\\n\"\n",
    "\n",
    "    prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|>\\n\"\n",
    "    prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a93493a-dc45-4956-aa5d-0d7b7b0501a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI assistant.\n",
      "user\n",
      "What's the tallest mountain in the world?\n",
      "assistant\n",
      "The tallest mountain in the world is Mount Everest, located in the Himalayas on the border between Nepal and Tibet, China. Its height is approximately 8,848.86 meters (29,031.7 feet) above sea level.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful AI assistant.\"\n",
    "history = []  # Append turns like {'user': '...', 'assistant': '...'}\n",
    "user_input = \"What's the tallest mountain in the world?\"\n",
    "\n",
    "prompt = format_prompt(system, user_input, history)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Parse out just the assistant's response\n",
    "response = output_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
    "print(response)\n",
    "\n",
    "# Update history\n",
    "history.append({'user': user_input, 'assistant': response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e62381ff-5104-4ea5-a937-96d32609b1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': \"What's the tallest mountain in the world?\",\n",
       "  'assistant': \"system\\nYou are a helpful AI assistant.\\nuser\\nWhat's the tallest mountain in the world?\\nassistant\\nThe tallest mountain in the world is Mount Everest, located in the Himalayas on the border between Nepal and Tibet, China. Its height is approximately 8,848.86 meters (29,031.7 feet) above sea level.\"}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7a647-a1c6-4f80-a87f-d1cd3d261a59",
   "metadata": {},
   "source": [
    "Cleaning the space in GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2038502-15d0-4640-94dc-b2c4a1cd360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning GPU space\n",
    "\n",
    "del llm_model\n",
    "del tokenizer\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6662ca-a5dc-49db-80b1-4b5be1147dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: How are the distributed databases useful for Big data?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashb\\anaconda3\\envs\\rag\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 04 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are the distributed databases useful for Big data?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Distributed databases are particularly useful for handling Big Data because they can scale horizontally, allowing them to handle large volumes of data that can be too large for a single database to manage. Here are some reasons why distributed databases are useful for Big Data:\n",
      "\n",
      "1. **Scalability**: Distributed databases can scale horizontally, adding more nodes to the cluster as needed to handle increasing amounts of data. This allows them to handle large volumes of data that can't be handled by a single database.\n",
      "2. **High performance**: Distributed databases can distribute the load across multiple nodes, improving performance and reducing the time it takes to process and analyze large amounts of data.\n",
      "3. **Fault tolerance**: Distributed databases can continue to operate even if one or more nodes fail, reducing the risk of data loss and ensuring business continuity.\n",
      "4. **Cost-effective**: Distributed databases can be more cost-effective than traditional databases, as they can be built on commodity hardware and don't require the same level of maintenance and support.\n",
      "5. **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets create a simple template for prompt \n",
    "\n",
    "input_text = 'How are the distributed databases useful for Big data?'\n",
    "print(f\"user: {input_text}\\n\")\n",
    "\n",
    "#dialogue template for instruction tuned model\n",
    "\n",
    "dialogue = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":input_text\n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(conversation = dialogue, tokenize = False, add_generation_prompt = True)\n",
    "#print(f\"prompt(formatted): {prompt}\\n\")\n",
    "input_id = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "outputs = llm_model.generate(\n",
    "        **input_id,\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f\"llm: {output_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d1c26e-fc8d-41d2-8459-8bc73d4e49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='./llama-3-3b-instruct', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211095d0-e0fc-4121-8b17-f90af03471c4",
   "metadata": {},
   "source": [
    "## Augmenting prompt with context items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aa39a-0475-4086-8f5c-e2f1ca565c6f",
   "metadata": {},
   "source": [
    "Prompting techniques:\n",
    "1. give clear instructions\n",
    "2. give few examples of i/p / o/p\n",
    "3. give room to think\\\n",
    "using the function we created earlier where we put the query and get the value and index, then using that index to get the human \n",
    "readeable values from the list of dict we created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c1f911-079d-4ef4-a798-012ce75ed6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what is a way to connect Kafka Pipeline to data processing application?\n",
      "prompt:  Based on the following context items, please answer the query.\n",
      "    context_items:\n",
      "    - doi: 10.1145/568522.568525 [100] Adam Warski: “Kafka Streams – How Does It Fit the Stream Processing Landscape?,” softwaremill.com, June 1, 2016. Summary | 487\n",
      "- model as a table into which transactions can insert tuples, but which cannot be queried. The stream then consists of the log of tuples that committed transactions have written to this special table, in the order they were committed. External consumers can asynchronously consume this log and use it to update derived data systems. Kafka Connect [41] is an effort to integrate change data capture tools for a wide range of database systems with Kafka. Once the stream of change events is in Kafka, it can be used to update derived data systems such as search indexes, and also feed into stream processing systems as discussed later in this chapter. Event Sourcing There are some parallels between the ideas we’ve discussed here and event sourcing, a technique that was developed in the domain-driven design (DDD) community [42, 43, 44]. We will discuss event sourcing briefly, because it incorporates some useful and relevant ideas for streaming systems. Similarly to change data capture, event sourcing involves storing all changes to the application state as a log of change events. The biggest difference is that event sourcing applies the idea at a different level of abstraction: • In change data capture, the application uses the database in a mutable way, updating and deleting records at will. The log of changes is extracted from the database at a low level (e.g., by parsing the replication log), which ensures that the order of writes extracted from the database matches the order in which they were actually written, avoiding the race condition in Figure 11-4.\n",
      "- 15] Sanjay Aiyagari, Matthew Arrott, Mark Atwell, et al.: “ AMQP: Advanced Message Queuing Protocol Specification,” Version 0-9-1, November 2008. [ 16] “Google Cloud Pub/Sub: A Google-Scale Messaging Service,” cloud.google.com, 2016. [ 17] “Apache Kafka 0.9 Documentation,” kafka.apache.org, November 2015. [ 18] Jay Kreps, Neha Narkhede, and Jun Rao: “Kafka: A Distributed Messaging System for Log Processing,” at 6th International Workshop on Networking Meets Databases (NetDB), June 2011. [ 19] “Amazon Kinesis Streams Developer Guide,” docs.aws.amazon.com, April 2016. [ 20] Leigh Stewart and Sijie Guo: “Building DistributedLog: Twitter’s HighPerformance Replicated Log Service,” blog.twitter.com, September 16, 2015. [ 21] “DistributedLog Documentation,” Twitter, Inc., distributedlog.io, May 2016. [ 22] Jay Kreps: “Benchmarking Apache Kafka: 2 Million Writes Per Second (On Three Cheap Machines),” engineering.linkedin.com, April 27, 2014. [ 23] Kartik Paramasivam: “How We’re Improving and Advancing Kafka at LinkedIn,” engineering.linkedin.com, September 2, 2015. [\n",
      "- doi: 10.1145/2463676.2465272 [86] Martin Kleppmann: “Samza Newsfeed Demo,” github.com, September 2014. [ 87] Ben Kirwin: “Doing the Impossible: Exactly-Once Messaging Patterns in Kafka,” ben.kirw.in, November 28, 2014. [ 88] Pat Helland: “Data on the Outside Versus Data on the Inside,” at 2nd Biennial Conference on Innovative Data Systems Research (CIDR), January 2005. [ 89] Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd edition. John Wiley & Sons, 2013. ISBN: 978-1-118-53080-1 [90] Viktor Klang: “I’m coining the phrase ‘effectively-once’ for message processing with at-least-once + idempotent operations,” twitter.com, October 20, 2016. [ 91] Matei Zaharia, Tathagata Das, Haoyuan Li, et al.: “ Discretized Streams: An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters,” at 4th USENIX Conference in Hot Topics in Cloud Computing (HotCloud), June 2012. [ 92] Kostas Tzoumas, Stephan Ewen, and Robert Metzger: “High-Throughput, LowLatency, and Exactly-Once Stream Processing with Apache Flink,” data-artisans.com, August 5, 2015. 486 | Chapter 11: Stream Processing\n",
      "- 29th European Conference on Object-Oriented Programming (ECOOP), July 2015. doi:10.4230/LIPIcs. ECOOP.2015.568 [44] Mark Soper: “Clearing Up React Data Management Confusion with Flux, Redux, and Relay,” medium.com, December 3, 2015. [ 45] Eno Thereska, Damian Guy, Michael Noll, and Neha Narkhede: “Unifying Stream Processing and Interactive Queries in Apache Kafka,” confluent.io, October 26, 2016. [ 46] Frank McSherry: “Dataflow as Database,” github.com, July 17, 2016. [ 47] Peter Alvaro: “I See What You Mean,” at Strange Loop, September 2015. [ 48] Nathan Marz: “Trident: A High-Level Abstraction for Realtime Computation,” blog.twitter.com, August 2, 2012. [ 49] Edi Bice: “Low Latency Web Scale Fraud Prevention with Apache Samza, Kafka and Friends,” at Merchant Risk Council MRC Vegas Conference, March 2016. [ 50] Charity Majors: “The Accidental DBA,” charity.wtf, October 2, 2016. [ 51] Arthur J. Bernstein, Philip M. Lewis, and Shiyong Lu: “Semantic Conditions for Correctness at Different Isolation Levels,” at 16th International Conference on Data Engineering (ICDE), February 2000.\n",
      "    query:what is a way to connect Kafka Pipeline to data processing application?\n",
      "    answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def prompt_formatter(query:str, context_items: list[dict]) -> str:\n",
    "    context = '- ' + '\\n- '.join(item['chunk_paragraph'] for item in context_items)\n",
    "    base_prompt = \"\"\" Based on the following context items, please answer the query.\n",
    "    context_items:\n",
    "    {context}\n",
    "    query:{query}\n",
    "    answer:\n",
    "    \"\"\"\n",
    "    prompt = base_prompt.format(context = context, query = query)\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "query = 'How does distributed databases help in big data processing?'\n",
    "query = 'what is a way to connect Kafka Pipeline to data processing application?'\n",
    "\n",
    "scores, index = retrieve_data(query=query, k_resources_return =5, embeddings= text_chunk_embeddings, print_time = False)\n",
    "context_items = [chunk_filtered_collection[i] for i in index]\n",
    "\n",
    "prompt = prompt_formatter(query, context_items)\n",
    "print(f\"query: {query}\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f69e607-2972-4b7d-ab60-85c63331b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\m'\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what is a way to connect Kafka Pipeline to data processing application?\n",
      "rag_answer:\\m<|begin_of_text|> There are several ways to connect a Kafka pipeline to a data processing application. Here are a few options:\n",
      "\n",
      "     1.  **Change Data Capture (CDC)**: Kafka Connect can be used to integrate change data capture tools with Kafka. This allows the stream of change events to be used to update derived data systems such as search indexes and feed into stream processing systems.\n",
      "\n",
      "     2.  **Event Sourcing**: Event sourcing involves storing all changes to the application state as a log of change events. This can be used to implement a data processing pipeline that can be used to update derived data systems.\n",
      "\n",
      "     3.  **Apache Flink**: Apache Flink is a stream processing framework that can be used to process data in real-time. It can be used to implement a data processing pipeline that can be connected to a Kafka pipeline.\n",
      "\n",
      "     4.  **Apache Samza**: Apache Samza is a stream processing framework that can be used to process data in real-time. It can be used to implement a data processing pipeline that can be connected to a Kafka pipeline.\n",
      "\n",
      "     5.  **Trident**: Trident is a high-level abstraction for real-time computation. It can be used to implement a data processing pipeline that can be\n",
      "CPU times: total: 38 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = llm_model.generate(**input_ids, temperature=0.7, do_sample=True, max_new_tokens=250)\n",
    "outputs_text= tokenizer.decode(outputs[0])\n",
    "print(f\"query: {query}\")\n",
    "print(f\"rag_answer:\\m{outputs_text.replace(prompt, '')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc2cd80e-839b-4d01-b677-0aa13365a08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:47: SyntaxWarning: invalid escape sequence '\\m'\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Can you give some info on Graph Databases?\n",
      "rag_answer:\\m<|begin_of_text|>Graph Databases are a type of NoSQL database that is optimized for handling complex data relationships and queries. They consist of two main types of objects: vertices (also known as nodes or entities) and edges (also known as relationships or arcs). Each vertex represents a unique entity, such as a person, web page, or junction, while edges represent the relationships between these entities. Graph databases provide a flexible and dynamic way to store and query data, making them suitable for applications with complex relationships, such as social networks, web graphs, and road or rail networks. They support various query languages, including Cypher, SPARQL, and Datalog, and are particularly useful for evolvability, allowing easy extension of the database as the application's data structure evolves. Graph databases are particularly useful in applications where there are many-to-many relationships, and can be used to model complex data structures, such as social networks, web graphs, and road or rail networks. They also support various features, such as query optimization, data compression, and data deduplication.<|eot_id|>\n",
      "CPU times: total: 38.9 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def prompt_formatter2(query:str, context_items: list[dict]) -> str:\n",
    "    context = '- ' + '\\n- '.join(item['chunk_paragraph'] for item in context_items)\n",
    "    base_prompt = \"\"\" Based on the following context items, please answer the query.\n",
    "    give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Dont return the thinking only return the answer.\n",
    "    Make sure your answers are as explanatory as possible.\n",
    "    Use the following examples as the ideal answer style\n",
    "    \\nExample 1:\n",
    "    Query: What drove the adoption of NoSQL Databases?\n",
    "    Answer: A need for greater scalability than relational databases can easily achieve, including very large datasets or very high write throughput, a widespread preference for free and open source software over commercial database products, specialized query operations that are not well supported by the relational model and frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model are some of the driving forces that let to adaptation of NoSQL Databases.\n",
    "    \\nExample 2:\n",
    "    Query: Can you give some info on Message Brokers?\n",
    "    Answer: Message Brokers are widely used alternative is to send messages (also known as a  message queue), which is essentially a kind of database that is optimized for handling message streams. It runs as a server, with producers and consumers connecting  to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker.\n",
    "    Now use the following context item to answer the query\n",
    "    context_items:\n",
    "    {context}\n",
    "    query:{query}\n",
    "    answer:\n",
    "    \"\"\"\n",
    "    base_prompt = base_prompt.format(context = context, query = query)\n",
    "    #dialogue template for instruction tuned model\n",
    "\n",
    "    dialogue = [\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":base_prompt\n",
    "        }\n",
    "    ]\n",
    "    #Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue, tokenize=False, add_generation_prompt=True) #will tokenize it later like last time\n",
    "    return prompt\n",
    "\n",
    "query = 'Can you give some info on Graph Databases?'\n",
    "\n",
    "scores, index = retrieve_data(query=query, k_resources_return =5, embeddings= text_chunk_embeddings, print_time = False)\n",
    "context_items = [chunk_filtered_collection[i] for i in index]\n",
    "\n",
    "prompt = prompt_formatter2(query, context_items)\n",
    "#print(prompt)\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = llm_model.generate(**input_ids, temperature=0.7, do_sample=True, max_new_tokens=250)\n",
    "outputs_text= tokenizer.decode(outputs[0])\n",
    "print(f\"query: {query}\")\n",
    "print(f\"rag_answer:\\m{outputs_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13a35f-5646-401c-8aad-07857d2b533d",
   "metadata": {},
   "source": [
    "Still need to understand more about the dialogue for chat style\\\n",
    "\\\n",
    "Creating the function to play around in chat like structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87f241-8057-4c4c-bcb1-9bb7b8cd2824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
